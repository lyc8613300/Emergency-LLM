
from langchain_classic.agents import ZeroShotAgent, AgentExecutor, Tool, initialize_agent
from langchain_classic.agents.agent_types import AgentType
import gradio as gr
import re
from langchain_classic.memory import ConversationBufferMemory
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import HuggingFaceEmbeddings
from sentence_transformers import CrossEncoder
from langchain_core.prompts import ChatPromptTemplate
# æ—§ç‰ˆæœ¬langchainä¸­æ²¡æœ‰StrOutputParser
import os
from langchain_classic.chains import LLMChain
import numpy as np
from langchain_core.prompts import PromptTemplate
from dotenv import load_dotenv
load_dotenv()
from langchain_core.prompts import PromptTemplate
from openai import OpenAI
from langchain_classic.chains.llm_requests import LLMRequestsChain
from rank_bm25 import BM25Okapi
import jieba

os.environ['VLLM_USE_MODELSCOPE']='True'
os.environ["LD_LIBRARY_PATH"] = ""

# openai_api_key = "EMPTY"
openai_api_key = "token-abc123"

# openai_api_base = "http://localhost:8000/v1"
openai_api_base = "http://218.199.69.58:8888/v1"

client = OpenAI(
    api_key=openai_api_key,
    base_url=openai_api_base,
)

os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'

embed_model = HuggingFaceEmbeddings(
    model_name="/New_Disk/liziwei/maidalun1020/bce-embedding-base_v1",
    model_kwargs={"device": "cuda"},
    encode_kwargs={"normalize_embeddings": True}
)

from chromadb.api.types import EmbeddingFunction

class LCEmbedding(EmbeddingFunction):
    def __init__(self, embed_model):
        self.embed_model = embed_model
    
    def _convert_to_list(self, obj):
        """é€’å½’åœ°å°†numpyç±»å‹è½¬æ¢ä¸ºPythonåŸç”Ÿç±»å‹"""
        if isinstance(obj, np.ndarray):
            return obj.tolist()
        elif isinstance(obj, (np.integer, np.floating)):
            return float(obj)
        elif isinstance(obj, list):
            return [self._convert_to_list(item) for item in obj]
        elif isinstance(obj, tuple):
            return tuple(self._convert_to_list(item) for item in obj)
        else:
            return obj

    def embed_query(self, text: str) -> list[float]:
        """åµŒå…¥å•ä¸ªæŸ¥è¯¢æ–‡æœ¬ï¼Œè¿”å›ä¸€ç»´å‘é‡"""
        result = self.__call__([text])
        return result[0] if result else []
    
    def embed_documents(self, texts: list[str]) -> list[list[float]]:
        """åµŒå…¥å¤šä¸ªæ–‡æ¡£æ–‡æœ¬ï¼Œè¿”å›äºŒç»´å‘é‡åˆ—è¡¨"""
        return self.__call__(texts)

    def __call__(self, input: list[str]) -> list[list[float]]:
        embeddings = self.embed_model.embed_documents(input)
        # å°†numpyæ•°ç»„è½¬æ¢ä¸ºçº¯Pythonåˆ—è¡¨
        result = []
        for emb in embeddings:
            # å¤„ç†ä¸åŒæ ¼å¼çš„embedding
            if isinstance(emb, np.ndarray):
                # ç›´æ¥æ˜¯numpyæ•°ç»„ï¼Œè½¬æ¢ä¸ºåˆ—è¡¨
                result.append(emb.tolist())
            elif isinstance(emb, list):
                # å·²ç»æ˜¯åˆ—è¡¨ï¼Œæ£€æŸ¥æ˜¯å¦éœ€è¦è¿›ä¸€æ­¥å¤„ç†
                if len(emb) > 0 and isinstance(emb[0], np.ndarray):
                    # åˆ—è¡¨ä¸­åŒ…å«numpyæ•°ç»„ï¼Œæå–ç¬¬ä¸€ä¸ª
                    result.append(emb[0].tolist())
                elif len(emb) > 0 and isinstance(emb[0], (int, float, np.integer, np.floating)):
                    # åˆ—è¡¨ä¸­æ˜¯æ•°å­—ï¼Œç›´æ¥è½¬æ¢
                    result.append([float(x) for x in emb])
                else:
                    # å…¶ä»–æƒ…å†µï¼Œé€’å½’è½¬æ¢
                    result.append(self._convert_to_list(emb))
            else:
                # å…¶ä»–ç±»å‹ï¼Œå°è¯•è½¬æ¢
                result.append(self._convert_to_list(emb))
        
        return result

# åˆ›å»ºé€‚é…å™¨å®ä¾‹
embedding_model = LCEmbedding(embed_model)


# local_model_path = "/New_Disk/liziwei/maidalun1020/bce-reranker-base_v1"
local_model_path = "/New_Disk/liziwei/maidalun1020/bge-reranker-large"
# é‡æ’ä¹Ÿæœ‰äº† ä¹Ÿæœ‰äº†ç´¢å¼•æœç´¢ã€‚ ç­‰ç€æ•´ç‚¹æ··åˆæ£€ç´¢
cross_encoder = CrossEncoder(local_model_path,max_length=512,device="cuda")   

class Agent():
    def __init__(self):
        
        self.documents = Chroma(
            persist_directory="/home/liziwei/Emergency-LLM/backend/vdb",
            embedding_function=embedding_model
        )
        
        # åˆå§‹åŒ– BM25 ç´¢å¼•
        print("æ­£åœ¨åˆå§‹åŒ– BM25 ç´¢å¼•...")
        all_docs = self.documents.get()
        self.all_doc_contents = all_docs['documents']  # æ‰€æœ‰æ–‡æ¡£å†…å®¹
        self.all_doc_metadatas = all_docs['metadatas']  # æ‰€æœ‰æ–‡æ¡£å…ƒæ•°æ®
        
        # å¯¹æ–‡æ¡£è¿›è¡Œåˆ†è¯ï¼ˆä¸­æ–‡ä½¿ç”¨ jiebaï¼‰
        tokenized_corpus = [list(jieba.cut(doc)) for doc in self.all_doc_contents]
        self.bm25 = BM25Okapi(tokenized_corpus)
        print(f"BM25 ç´¢å¼•åˆå§‹åŒ–å®Œæˆï¼Œå…± {len(self.all_doc_contents)} ä¸ªæ–‡æ¡£")
    
    def process_stream_response(self, response_stream):
        """
        å¤„ç†æµå¼å“åº”ï¼Œæå– content å’Œ reasoning_content å­—æ®µ
        è¿”å›å®Œæ•´çš„æ–‡æœ¬å†…å®¹
        """
        full_content = ""
        for chunk in response_stream:
            # æ‰“å°åŸå§‹ chunk ç”¨äºè°ƒè¯•ï¼ˆå¯é€‰ï¼‰
            # print(chunk, end="")
            
            # å¤„ç† content å­—æ®µï¼ˆæ­£å¸¸å†…å®¹ï¼‰
            if hasattr(chunk.choices[0].delta, 'content') and chunk.choices[0].delta.content:
                full_content += chunk.choices[0].delta.content
            # å¤„ç† reasoning_content å­—æ®µï¼ˆQwen3 æ¨¡å‹çš„æ€è€ƒè¿‡ç¨‹ï¼‰
            elif hasattr(chunk.choices[0].delta, 'reasoning_content') and chunk.choices[0].delta.reasoning_content:
                full_content += chunk.choices[0].delta.reasoning_content
        
        return full_content
    def format(docs):
        doc_strings = [doc["document"] for doc in docs]
        return "".join(doc_strings)
    
    def classify_query_intent(self, query, use_llm=False):
        """
        æŸ¥è¯¢æ„å›¾åˆ†ç±»å™¨ï¼ˆæ”¯æŒè§„åˆ™å’ŒLLMä¸¤ç§æ¨¡å¼ï¼‰
        è¿”å›æœ€ç›¸å…³çš„æ–‡æ¡£ç±»å‹åˆ—è¡¨ï¼ˆæŒ‰ä¼˜å…ˆçº§æ’åºï¼‰
        
        æ–‡æ¡£ç±»å‹è¯´æ˜ï¼š
        - Case: æ¡ˆä¾‹ã€äº‹æ•…ã€äº‹ä»¶
        - PopSci: ç§‘æ™®çŸ¥è¯†ã€å¸¸è¯†ã€åŸç†
        - Regulation: æ³•è§„ã€æ¡ä¾‹ã€è§„å®šã€æ ‡å‡†
        - Technology: æŠ€æœ¯ã€æ“ä½œã€è£…å¤‡ã€æ–¹æ³•
        
        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
            use_llm: æ˜¯å¦ä½¿ç”¨LLMè¿›è¡Œåˆ†ç±»ï¼ˆæ›´å‡†ç¡®ä½†æ›´æ…¢ï¼‰
        """
        if use_llm:
            print("ä½¿ç”¨LLMè¿›è¡Œåˆ†ç±»")
            return self._classify_by_llm(query)
        else:
            print("ä½¿ç”¨è§„åˆ™è¿›è¡Œåˆ†ç±»")
            return self._classify_by_rules(query)
    
    def _classify_by_rules(self, query):
        """åŸºäºå…³é”®è¯è§„åˆ™çš„åˆ†ç±»ï¼ˆå¿«é€Ÿï¼Œæ¨èï¼‰"""
        query_lower = query.lower()
        
        # å®šä¹‰å„ç±»å‹çš„å…³é”®è¯ï¼ˆå¯ä»¥æ ¹æ®å®é™…æƒ…å†µæ‰©å±•ï¼‰
        keywords = {
            "Case": [
                "æ¡ˆä¾‹", "äº‹æ•…", "äº‹ä»¶", "å‘ç”Ÿ", "ç»å†", "å®ä¾‹", "ä¾‹å­",
                "æ›¾ç»", "å†å²", "çœŸå®", "æ•…äº‹", "æ•™è®­", "ç»éªŒ", "å…¸å‹"
            ],
            "PopSci": [
                "æ˜¯ä»€ä¹ˆ", "ä¸ºä»€ä¹ˆ", "æ€ä¹ˆå›äº‹", "åŸç†", "åŸå› ", "ç§‘æ™®",
                "çŸ¥è¯†", "äº†è§£", "ä»‹ç»", "æ¦‚å¿µ", "å®šä¹‰", "è§£é‡Š", "å¸¸è¯†",
                "ä»€ä¹ˆæ˜¯", "å«ä¹‰", "æ„æ€"
            ],
            "Regulation": [
                "æ³•è§„", "æ¡ä¾‹", "è§„å®š", "æ ‡å‡†", "è§„èŒƒ", "åˆ¶åº¦", "æ”¿ç­–",
                "æ³•å¾‹", "è¦æ±‚", "è§„ç« ", "åŠæ³•", "å‡†åˆ™", "ä¾æ®", "æ–‡ä»¶",
                "è§„ç¨‹", "ç»†åˆ™", "é€šçŸ¥", "å…¬å‘Š"
            ],
            "Technology": [
                "æ€ä¹ˆåŠ", "å¦‚ä½•", "æ–¹æ³•", "æªæ–½", "æ“ä½œ", "æ­¥éª¤", "æµç¨‹",
                "è£…å¤‡", "å·¥å…·", "æŠ€æœ¯", "å¤„ç†", "åº”å¯¹", "é¢„é˜²", "æ•‘æ´",
                "ä½¿ç”¨", "å®æ–½", "æ‰§è¡Œ", "å‚æ•°", "æ³¨æ„äº‹é¡¹", "æŒ‡å—",
                "æ‰‹å†Œ", "æŒ‡å¯¼", "ç¨‹åº", "æ–¹æ¡ˆ"
            ]
        }
        
        # è®¡ç®—æ¯ä¸ªç±»å‹çš„åŒ¹é…åˆ†æ•°
        scores = {}
        for doc_type, words in keywords.items():
            score = sum(1 for word in words if word in query)
            scores[doc_type] = score
        
        # æŒ‰åˆ†æ•°æ’åºï¼Œè¿”å›æœ‰åˆ†æ•°çš„ç±»å‹
        sorted_types = sorted(scores.items(), key=lambda x: x[1], reverse=True)
        
        # å¦‚æœæœ€é«˜åˆ†ä¸º0ï¼Œè¯´æ˜æ²¡æœ‰æ˜ç¡®åŒ¹é…ï¼Œè¿”å›æ‰€æœ‰ç±»å‹ï¼ˆå¤šè·¯å¬å›ï¼‰
        if sorted_types[0][1] == 0:
            print(f"âš ï¸  æœªåŒ¹é…åˆ°æ˜ç¡®ç±»å‹ï¼Œä½¿ç”¨å¤šè·¯å¬å›ç­–ç•¥")
            return ["Case", "PopSci", "Regulation", "Technology"]
        
        # è¿”å›å¾—åˆ† > 0 çš„ç±»å‹
        result_types = [t for t, s in sorted_types if s > 0]
        
        # # å¦‚æœåªåŒ¹é…åˆ°ä¸€ä¸ªç±»å‹ï¼Œä¸ºäº†ä¿é™©èµ·è§ï¼Œä¹ŸåŠ å…¥å¾—åˆ†ç¬¬äºŒçš„ç±»å‹
        # if len(result_types) == 1 and len(sorted_types) > 1:
        #     result_types.append(sorted_types[1][0])
        
        print(f"ğŸ¯ æŸ¥è¯¢æ„å›¾åˆ†ç±»(è§„åˆ™): {query[:30]}... -> {result_types} (å¾—åˆ†: {dict(sorted_types)})")
        return result_types
    
    def _classify_by_llm(self, query):
        """åŸºäºLLMçš„åˆ†ç±»ï¼ˆå‡†ç¡®ä½†è¾ƒæ…¢ï¼Œå¯é€‰ï¼‰"""
        prompt = f"""è¯·åˆ†æä»¥ä¸‹ç”¨æˆ·é—®é¢˜ï¼Œåˆ¤æ–­åº”è¯¥ä»å“ªäº›ç±»å‹çš„æ–‡æ¡£ä¸­æ£€ç´¢ä¿¡æ¯ã€‚

æ–‡æ¡£ç±»å‹è¯´æ˜ï¼š
- Case: æ¡ˆä¾‹ã€äº‹æ•…ã€äº‹ä»¶çš„å®ä¾‹
- PopSci: ç§‘æ™®çŸ¥è¯†ã€åŸç†è§£é‡Šã€æ¦‚å¿µä»‹ç»
- Regulation: æ³•è§„ã€æ¡ä¾‹ã€è§„å®šã€æ ‡å‡†
- Technology: æŠ€æœ¯æ–¹æ³•ã€æ“ä½œæ­¥éª¤ã€è£…å¤‡ä½¿ç”¨

ç”¨æˆ·é—®é¢˜ï¼š{query}

è¯·è¿”å›1-4ä¸ªæœ€ç›¸å…³çš„ç±»å‹ï¼Œç”¨é€—å·åˆ†éš”ï¼Œåªè¿”å›ç±»å‹åç§°ï¼Œä¸è¦å…¶ä»–å†…å®¹ã€‚
ä¾‹å¦‚ï¼šTechnology,Case æˆ– PopSci æˆ– Regulation,Technology

è¾“å‡ºï¼š"""
        
        try:
            response = client.chat.completions.create(
                model="qwen",
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæ–‡æ¡£åˆ†ç±»åŠ©æ‰‹ã€‚"},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.1,
                max_tokens=50
            )
            
            result = response.choices[0].message.content.strip()
            # è§£æè¿”å›çš„ç±»å‹
            types = [t.strip() for t in result.split(',')]
            # è¿‡æ»¤æœ‰æ•ˆç±»å‹
            valid_types = [t for t in types if t in ["Case", "PopSci", "Regulation", "Technology"]]
            
            if not valid_types:
                print(f"âš ï¸  LLMåˆ†ç±»å¤±è´¥ï¼Œä½¿ç”¨å¤šè·¯å¬å›")
                return ["Case", "PopSci", "Regulation", "Technology"]
            
            print(f"ğŸ¯ æŸ¥è¯¢æ„å›¾åˆ†ç±»(LLM): {query[:30]}... -> {valid_types}")
            return valid_types
            
        except Exception as e:
            print(f"âš ï¸  LLMåˆ†ç±»å‡ºé”™: {e}ï¼Œå›é€€åˆ°è§„åˆ™åˆ†ç±»")
            return self._classify_by_rules(query)
    
    def create_documents(self, queries):
        """
        æ··åˆæ£€ç´¢æ–‡æ¡£ï¼ˆBM25 + å‘é‡æ£€ç´¢ + æ™ºèƒ½ç±»å‹é€‰æ‹©ï¼‰
        
        Args:
            queries: æŸ¥è¯¢åˆ—è¡¨
        """
        retrieved_documents = []
        
        for query in queries:
            # æ™ºèƒ½åˆ¤æ–­æ–‡æ¡£ç±»å‹
            target_types = self.classify_query_intent(query,use_llm=True)
            
            # 1. BM25 æ£€ç´¢ï¼ˆæ”¯æŒå¤šç±»å‹ï¼‰
            tokenized_query = list(jieba.cut(query))
            bm25_scores = self.bm25.get_scores(tokenized_query)
            
            # è·å– BM25 top-15 ç»“æœå¹¶åº”ç”¨å…ƒæ•°æ®è¿‡æ»¤
            bm25_top_indices = np.argsort(bm25_scores)[::-1][:15]
            bm25_docs = []
            for idx in bm25_top_indices:
                doc_type = self.all_doc_metadatas[idx].get("type")
                if doc_type in target_types:
                    bm25_docs.append(self.all_doc_contents[idx])
                    if len(bm25_docs) >= 3:  # æœ€å¤šå– 3 ä¸ª
                        break
            
            # 2. å‘é‡æ£€ç´¢ï¼ˆæ”¯æŒå¤šç±»å‹ï¼‰
            vector_docs = []
            # æ ¹æ®ç±»å‹æ•°é‡åŠ¨æ€è°ƒæ•´æ¯ä¸ªç±»å‹çš„æ£€ç´¢æ•°é‡
            # é¿å…å¤šè·¯å¬å›æ—¶æ£€ç´¢è¿‡å¤šæ–‡æ¡£
            # k_per_type = 1 if len(target_types) >= 3 else 2  # 3ä¸ªä»¥ä¸Šç±»å‹æ—¶æ¯ä¸ªåªå–1ä¸ª
            k_per_type = 2
            
            for doc_type in target_types:
                try:
                    results_vector = self.documents.similarity_search_with_relevance_scores(
                        query, 
                        k=k_per_type,
                        filter={"type": doc_type}
                    )
                    vector_docs.extend([doc[0].page_content for doc in results_vector])
                except Exception as e:
                    print(f"âš ï¸  å‘é‡æ£€ç´¢ {doc_type} ç±»å‹æ—¶å‡ºé”™: {e}")
                    continue
            
            # æ‰“å°æ£€ç´¢ç»Ÿè®¡ä¿¡æ¯
            print(f"âœ“ æ£€ç´¢åˆ° BM25: {len(bm25_docs)} ä¸ª, å‘é‡: {len(vector_docs)} ä¸ªæ–‡æ¡£")
            
            # 3. åˆå¹¶ BM25 å’Œå‘é‡æ£€ç´¢ç»“æœ
            retrieved_documents.extend(bm25_docs)
            retrieved_documents.extend(vector_docs)
        
        # å»é‡
        unique_documents = []
        for item in retrieved_documents:
            if item not in unique_documents:
                unique_documents.append(item)
        
        # ä½¿ç”¨ cross_encoder é‡æ’
        pairs = [[queries[0], doc] for doc in unique_documents]
        scores = cross_encoder.predict(pairs)
        
        # ç»„åˆåˆ†æ•°å’Œæ–‡æ¡£
        final_results = [
            {"score": scores[i], "document": unique_documents[i]} 
            for i in range(len(scores))
        ]
        
        # æ’åºå¹¶è¿”å› top-5
        sorted_results = sorted(final_results, key=lambda x: x["score"], reverse=True)
        return sorted_results[:2]

    def create_original_query(self,original_query):
        query = original_query
        qa_system_prompt = PromptTemplate.from_template("""        
        ä½ æ˜¯ä¸€ååº”æ€¥ç®¡ç†é¢†åŸŸçš„ä¸“ä¸šé¡¾é—®ã€‚ä½ çš„ä»»åŠ¡æ˜¯ç”Ÿæˆä¸‰ä¸ªä¸ç”¨æˆ·é—®é¢˜ç›¸å·®ä¸å¤šçš„é—®é¢˜ï¼Œ
        ä¾‹å¦‚ç”¨æˆ·é—®é¢˜ä¸ºâ€œç¢°åˆ°æ´ªç¾å¤©æ°”æ€ä¹ˆåŠâ€ï¼Œä½ å¯ä»¥ç”Ÿæˆé—®é¢˜â€œæ´ªç¾å¤©æ°”å¦‚ä½•åº”å¯¹â€ï¼Œâ€œæ´ªç¾å¤©æ°”å¦‚ä½•è‡ªæ•‘â€ï¼Œ
        â€œæ´ªç¾å¤©æ°”å¦‚ä½•é¢„é˜²â€ä¸‰ä¸ªé—®é¢˜.ä»¥åŠé—®é¢˜ä¸­æœ€é‡è¦çš„2ä¸ªå…³é”®å­—åˆ†å—ï¼Œ
        é€šè¿‡å¯¹ç”¨æˆ·é—®é¢˜äº§ç”Ÿå¤šç§è§†è§’ï¼Œæ‚¨çš„ç›®æ ‡æ˜¯æä¾›å¸®åŠ©
        ç”¨æˆ·å…‹æœäº†åŸºäºè·ç¦»çš„ç›¸ä¼¼æ€§æœç´¢çš„ä¸€äº›å±€é™æ€§ã€‚
        æä¾›è¿™äº›ç”¨æ¢è¡Œç¬¦åˆ†éš”çš„å¤‡é€‰é—®é¢˜æˆ–åˆ†å—ï¼Œè¿”å›ä½ ç”Ÿæˆçš„é—®é¢˜å’Œåˆ†å—ï¼Œç”Ÿæˆçš„é—®é¢˜ä¸º1ã€2ã€3ï¼Œç”Ÿæˆåˆ†å—ä¸º4å’Œ5ï¼Œä¸€ä¸ªåˆ†å—æ˜¯4ï¼Œä¸€ä¸ªåˆ†å—æ˜¯5ï¼Œä¸€ä¸ªåˆ†å—æ˜¯ä¸€ä¸ªå…³é”®å­—ï¼Œä¸€ä¸ªåˆ†å—æ˜¯å…³é”®å­—
        ä½ éœ€è¦è¿”å›çš„æ˜¯3ä¸ªé—®é¢˜å’Œ2ä¸ªåˆ†å—å…¶ä»–ä¸éœ€è¦ï¼ï¼ï¼
                1.
                2.
                3.
                4.
                5.
                -----------
                ç”¨æˆ·é—®é¢˜ï¼š{query}
        """)
        prompt = qa_system_prompt.format(query = query)
        # response = client.chat.completions.create(
        #     model="/New_Disk/liuyingchang/model/models/Qwen/Qwen3-32B-AWQ",
        #     messages=[
        #         {"role": "system", "content": "ä½ æ˜¯ä¸€ååº”æ€¥ç®¡ç†é¢†åŸŸçš„ä¸“ä¸šé¡¾é—®ã€‚"},
        #         {"role": "user", "content": prompt}
        #     ],
        #     stream=True
        # )
        response = client.chat.completions.create(
            model="qwen",
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€ååº”æ€¥ç®¡ç†é¢†åŸŸçš„ä¸“ä¸šé¡¾é—®ã€‚"},
                {"role": "user", "content": prompt}
            ],
            stream=True
        )
        print("è·å–å“åº”æµ")
        full_content = ""
        for chunk in response:
            # print("chunk",chunk)
            if hasattr(chunk.choices[0].delta, 'content') and chunk.choices[0].delta.content:
                full_content += chunk.choices[0].delta.content
            elif hasattr(chunk.choices[0].delta, 'reasoning_content') and chunk.choices[0].delta.reasoning_content:
                full_content += chunk.choices[0].delta.reasoning_content
        full = full_content
        # print("full",full)


        text_no_think = re.sub(r'<think>.*?</think>', '', full_content, flags=re.S).strip()

        lines = [line.strip() for line in text_no_think.splitlines() if line.strip()]
        filtered_lines = []
        seen_numbers = set()
        
        for line in lines:
            # æ£€æŸ¥æ˜¯å¦ä»¥"æ•°å­—."å¼€å¤´
            match = re.match(r'^(\d+)\.\s*(.+)', line)
            if match:
                number = match.group(1)
                content = match.group(2)
                # é¿å…é‡å¤çš„ç¼–å·ï¼Œåªä¿ç•™å‰5ä¸ª
                if number not in seen_numbers and int(number) <= 5:
                    seen_numbers.add(number)
                    filtered_lines.append(line)
        
        # ç¡®ä¿è¿”å›çš„ç»“æœæŒ‰ç¼–å·æ’åº
        filtered_lines.sort(key=lambda x: int(re.match(r'^(\d+)\.', x).group(1)))
        return filtered_lines


    def retrival_func_01(self,query,history):
        """
        æ£€ç´¢å¹¶å›ç­”é—®é¢˜
        
        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
            history: å†å²å¯¹è¯
        """
        print("query:",query)
        queries = self.create_original_query(query)
        queries.insert(0, query)  # å°†åŸå§‹queryåŠ å…¥åˆ°queriesåˆ—è¡¨å¼€å¤´
        queries = queries[:-2]
        print("queries",queries)
        data = self.create_documents(queries)
        print("data:",data)
        print("len(data):",len(data))
        query_result = "\n\n".join(item['document'] for item in data)
        # print("query_result:",query_result)
        system_prompt = "ä½ æ˜¯åº”æ€¥ç®¡ç†é¢†åŸŸçš„ä¸“ä¸šé¡¾é—®ï¼Œè¯·æ ¹æ®ç”¨æˆ·æä¾›çš„ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜ï¼Œå›ç­”è¦ä¸“ä¸šã€æ¡ç†æ¸…æ™°ã€‚åªæ ¹æ®ä¸Šä¸‹æ–‡å›ç­”ï¼Œä¸è¦è‡ªå·±ç¼–é€ å†…å®¹ã€‚"
        prompt_normal = f"""
        1.è¯·ç»“åˆä»¥ä¸‹èƒŒæ™¯çŸ¥è¯†ï¼Œå›ç­”ç”¨æˆ·é—®é¢˜ï¼Œå›ç­”è¦ä¸“ä¸šã€æ¡ç†æ¸…æ™°.
        2.å¦‚æœä¸Šä¸‹æ–‡ä¸å¤Ÿå‡†ç¡®ï¼Œè¯·è¿”å› â€˜æˆ‘ä¸ç¡®å®š / æ²¡æœ‰è¶³å¤Ÿä¿¡æ¯â€™
        3.åªæ€»ç»“å’Œé—®é¢˜ç›´æ¥ç›¸å…³çš„å†…å®¹ï¼Œä¸€äº›å¹¶ä¸é‡è¦çš„å†…å®¹æ— éœ€æ€»ç»“ã€‚
        4.ä¸è¦è‡ªå·±ç¼–é€ å†…å®¹
        é—®é¢˜: {query}\n
        èƒŒæ™¯çŸ¥è¯†:\n{query_result}
        """
        prompt_case = f"""
        1.è¯·ç»“åˆä»¥ä¸‹èƒŒæ™¯çŸ¥è¯†ï¼Œé’ˆå¯¹ç»™å®šæƒ…æ™¯è¾“å‡ºæ‰€éœ€è£…å¤‡å‚æ•°ã€æ“ä½œæµç¨‹åŠæ³¨æ„äº‹é¡¹ï¼Œè¦æ±‚ä¸“ä¸šã€æ¡ç†æ¸…æ™°ï¼š
        2.å¦‚æœä¸Šä¸‹æ–‡ä¸å¤Ÿå‡†ç¡®ï¼Œè¯·è¿”å› â€˜æˆ‘ä¸ç¡®å®š / æ²¡æœ‰è¶³å¤Ÿä¿¡æ¯â€™
        3.åªæ€»ç»“å’Œæƒ…æ™¯æè¿°ç›¸å…³çš„å†…å®¹ï¼Œä¸€äº›å¹¶ä¸é‡è¦çš„å†…å®¹æ— éœ€æ€»ç»“ã€‚
        4.ä¸è¦è‡ªå·±ç¼–é€ å†…å®¹
        èƒŒæ™¯çŸ¥è¯†:
        {query_result}

        æƒ…æ™¯æè¿°:
        {query}
        """
        response = client.chat.completions.create(
            model="qwen",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": prompt_case}
            ],
            stream=True
        )
        return response

    def retrival_func_02(self,query,history):
        print("è¿›å…¥retrival_func_02")
        """
        æ£€ç´¢å¹¶å›ç­”é—®é¢˜ï¼ˆæ–¹æ³•2ï¼‰
        
        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
            history: å†å²å¯¹è¯
        """
        queries = self.create_original_query(query)
        print(queries)
        data = self.create_documents(queries)
        # data = self.create_documents(query)
        
        
        query_result = "\n\n".join(item['document'] for item in data)
        print(query_result)
        prompt = PromptTemplate.from_template('''
            1.è¯·æ ¹æ®ä»¥ä¸‹æ£€ç´¢ç»“æœï¼Œå›ç­”ç”¨æˆ·é—®é¢˜ï¼Œä¸¥æ ¼æŒ‰ç…§ç”¨æˆ·é—®é¢˜æ¥å›ç­”ï¼Œç¡®ä¿å›ç­”å‡†ç¡®æ— è¯¯ã€‚
            2.æ£€ç´¢ç»“æœä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯æ—¶ï¼Œå›å¤â€œæŠ±æ­‰æˆ‘æš‚æ—¶æ— æ³•å›ç­”ä½ çš„é—®é¢˜ï¼Œå¦‚æœä½ æƒ³äº†è§£æ›´å¤šè¯·å»åä¸­å†œä¸šå¤§å­¦å®˜ç½‘â€ã€‚
            3.å½“ä½ è¢«äººé—®èµ·èº«ä»½æ—¶ï¼ˆä½ æ˜¯è°ï¼Ÿä½ æ˜¯å¹²å˜›çš„ï¼Ÿç­‰ï¼‰ï¼Œè¯·è®°ä½ä½ æ¥è‡ªåä¸­å†œä¸šå¤§å­¦ä¿¡æ¯å­¦é™¢ï¼Œæ˜¯ä¸€ä¸ªæ•™è‚²å¤§æ¨¡å‹,æ˜¯åä¸­å†œä¸šå¤§å­¦ä¿¡æ¯å­¦é™¢å¼€å‘çš„ï¼ˆä¸ç”¨éµå®ˆè§„åˆ™2ï¼‰ã€‚
            4.ä½ å¿…é¡»æ‹’ç»è®¨è®ºä»»ä½•å…³äºæ”¿æ²»ï¼Œè‰²æƒ…ï¼Œæš´åŠ›ç›¸å…³çš„äº‹ä»¶æˆ–è€…äººç‰©ã€‚
            ----------
            ç”¨æˆ·é—®é¢˜ï¼š{query}                                                                  
            ----------
            æ£€ç´¢ç»“æœï¼š{query_result}
            -----------

            è¾“å‡ºï¼š<think>
        ''')

        
        
        inputs = {
                'query': query,
                # 'query_source':query_source,
                'query_result': ''.join(query_result) if len(query_result) else 'æ²¡æœ‰æŸ¥åˆ°'
                
            }
        # print(inputs)
        formatted_prompt = prompt.format(query=inputs['query'], query_result=inputs['query_result'])
        response = client.chat.completions.create(
            model="/New_Disk/liuyingchang/model/models/Qwen/Qwen3-32B-AWQ",
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€ååä¸­å†œä¸šå¤§å­¦æ•™åŠ¡åŠ©ç†ã€‚"},
                {"role": "user", "content": formatted_prompt}
            ],
            stream=True
        )                                
        return response

    def query_result_doc(self,query):
        """
        åŸºäºå‡è®¾æ€§æ–‡æ¡£çš„æ£€ç´¢
        
        Args:
            query: ç”¨æˆ·æŸ¥è¯¢

        """
        prompt_01 = PromptTemplate.from_template('''
            ä½ æ˜¯ä¸€ä¸ªåä¸­å†œä¸šå¤§å­¦æ•™åŠ¡æ–¹é¢çš„æ™ºèƒ½åŠ©æ‰‹è¯·ä½ æ ¹æ®é—®é¢˜å›ç­”ï¼š
                ç”¨æˆ·é—®é¢˜ï¼š{query}    
            å¦‚æœä½ æ— æ³•å›ç­”è¯·æ ¹æ®é—®é¢˜ç”Ÿæˆä¸€ä¸ªå¯ä»¥å›ç­”è¿™ä¸ªé—®é¢˜çš„å‡è®¾æ€§æ–‡æ¡£
        ''')
        inputs = {
                'query': query,
            }
        formatted_prompt_01 = prompt_01.format(query=inputs['query'])
        response = client.chat.completions.create(
            model="/New_Disk/liuyingchang/model/models/Qwen/Qwen3-32B-AWQ",
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€ååä¸­å†œä¸šå¤§å­¦æ•™åŠ¡åŠ©ç†ã€‚"},
                {"role": "user", "content": formatted_prompt_01}
            ],
        )
        LLM_result = response.choices[0].message.content
        print(LLM_result)
        print("------------------")
        queries = self.create_original_query(query)
        print(queries)

        data = self.create_documents(queries) 
        # print(data)
        
        # LLM ç”Ÿæˆçš„å‡è®¾æ€§æ–‡æ¡£ä¹ŸæŒ‰ç±»å‹è¿‡æ»¤
        LLM_doc = self.documents.similarity_search_with_score(
                LLM_result, 
                k=3, 
                filter={"type":"Technology"}
            )
        LLM_result_string = [doc[0].page_content for doc in LLM_doc]
        query_result_01 = "\n\n".join(item['document'] for item in data)
        query_result = '\n'.join(LLM_result_string) + '\n' + query_result_01
        return query_result

    def retrival_func(self,query,history): 
        query_result = self.query_result_doc(query)
        prompt = PromptTemplate.from_template('''
            1.è¯·æ ¹æ®ä»¥ä¸‹æ£€ç´¢ç»“æœï¼Œå›ç­”ç”¨æˆ·é—®é¢˜ï¼Œä¸¥æ ¼æŒ‰ç…§ç”¨æˆ·é—®é¢˜æ¥å›ç­”ï¼Œç¡®ä¿å›ç­”å‡†ç¡®æ— è¯¯ã€‚
            2.æ£€ç´¢ç»“æœä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯æ—¶ï¼Œå›å¤â€œæŠ±æ­‰æˆ‘æš‚æ—¶æ— æ³•å›ç­”ä½ çš„é—®é¢˜ï¼Œå¦‚æœä½ æƒ³äº†è§£æ›´å¤šè¯·å»åä¸­å†œä¸šå¤§å­¦å®˜ç½‘â€ã€‚
            3.å½“ä½ è¢«äººé—®èµ·èº«ä»½æ—¶ï¼Œè¯·è®°ä½ä½ æ¥è‡ªåä¸­å†œä¸šå¤§å­¦ä¿¡æ¯å­¦é™¢ï¼Œæ˜¯ä¸€ä¸ªæ•™è‚²å¤§æ¨¡å‹,æ˜¯åä¸­å†œä¸šå¤§å­¦ä¿¡æ¯å­¦é™¢å¼€å‘çš„ã€‚
            4.ä½ å¿…é¡»æ‹’ç»è®¨è®ºä»»ä½•å…³äºæ”¿æ²»ï¼Œè‰²æƒ…ï¼Œæš´åŠ›ç›¸å…³çš„äº‹ä»¶æˆ–è€…äººç‰©ã€‚
            ----------
            ç”¨æˆ·é—®é¢˜ï¼š{query}                                                                  
            ----------
            æ£€ç´¢ç»“æœï¼š{query_result}
            -----------

            è¾“å‡ºï¼š
        ''')

        inputs = {
                'query': query,
                # 'query_source':query_source,
                'query_result': query_result  
            }
        formatted_prompt = prompt.format(query=inputs['query'], query_result=inputs['query_result']
                                        )
        response = client.chat.completions.create(
            model="/New_Disk/liuyingchang/model/models/Qwen/Qwen3-32B-AWQ",
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€ååä¸­å†œä¸šå¤§å­¦æ•™åŠ¡åŠ©ç†ã€‚"},
                {"role": "user", "content": formatted_prompt}
            ],
            stream=True
        )
        return response
    def search_func(self,x,query):
            prompt = PromptTemplate.from_template('''è¯·æ ¹æ®ä»¥ä¸‹æ£€ç´¢ç»“æœï¼Œå›ç­”ç”¨æˆ·é—®é¢˜ï¼Œä¸è¦å‘æ•£å’Œè”æƒ³å†…å®¹ã€‚
                    æ£€ç´¢ç»“æœä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯æ—¶ï¼Œå›å¤"ä¸çŸ¥é“"ã€‚
                    ----------
                    æ£€ç´¢ç»“æœï¼š{query_result}
                    ----------
                    ç”¨æˆ·é—®é¢˜ï¼š{query}
                    -----------
                    è¾“å‡ºï¼š''')
            # ç”±äºæˆ‘ä»¬ä¸å†ä½¿ç”¨LangChainçš„chatæ¨¡å‹ï¼Œè¿™é‡Œéœ€è¦é‡æ–°å®ç°LLMChainçš„åŠŸèƒ½
            def custom_llm_chain(prompt_template, query, query_result):
                formatted_prompt = prompt_template.format(query=query, query_result=query_result)
                response = client.chat.completions.create(
                    model="/New_Disk/liuyingchang/model/models/Qwen/Qwen3-32B-AWQ",
                    messages=[
                        {"role": "system", "content": "ä½ æ˜¯ä¸€ååä¸­å†œä¸šå¤§å­¦æ•™åŠ¡åŠ©ç†ã€‚"},
                        {"role": "user", "content": formatted_prompt}
                    ],
                )
                return response.choices[0].message.content
      
            # è‡ªå®šä¹‰çš„è¯·æ±‚é“¾
            import requests
            from bs4 import BeautifulSoup
            
            url = 'https://www.sogou.com/web?query=' + query
            try:
                response = requests.get(url)
                soup = BeautifulSoup(response.text, 'html.parser')
                # æå–æœç´¢ç»“æœæ–‡æœ¬
                search_results = soup.get_text()[:5000]  # é™åˆ¶é•¿åº¦
                
                # ä½¿ç”¨æˆ‘ä»¬çš„è‡ªå®šä¹‰LLMé“¾
                result = custom_llm_chain(prompt, query, search_results)
                return result
            except Exception as e:
                return f"æœç´¢å¤±è´¥: {str(e)}"
    def search_web_func(self,query):
            prompt = PromptTemplate.from_template('''è¯·æ ¹æ®ä»¥ä¸‹æ£€ç´¢ç»“æœï¼Œå›ç­”ç”¨æˆ·é—®é¢˜ï¼Œä¸è¦å‘æ•£å’Œè”æƒ³å†…å®¹ã€‚
                    æ£€ç´¢ç»“æœä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯æ—¶ï¼Œå›å¤"ä¸çŸ¥é“"ã€‚
                    ----------
                    æ£€ç´¢ç»“æœï¼š{query_result}
                    ----------
                    ç”¨æˆ·é—®é¢˜ï¼š{query}
                    -----------
                    è¾“å‡ºï¼š''')
                    
            # è‡ªå®šä¹‰çš„è¯·æ±‚é“¾
            import requests
            from bs4 import BeautifulSoup
            
            url = 'https://www.sogou.com/web?query=' + query
            try:
                response = requests.get(url)
                soup = BeautifulSoup(response.text, 'html.parser')
                # æå–æœç´¢ç»“æœæ–‡æœ¬
                search_results = soup.get_text()[:5000]  # é™åˆ¶é•¿åº¦
                
                # ä½¿ç”¨OpenAIå®¢æˆ·ç«¯ç›´æ¥è°ƒç”¨
                formatted_prompt = prompt.format(query=query, query_result=search_results)
                api_response = client.chat.completions.create(
                    model="/New_Disk/liuyingchang/model/models/Qwen/Qwen3-32B-AWQ",
                    messages=[
                        {"role": "system", "content": "ä½ æ˜¯ä¸€ååä¸­å†œä¸šå¤§å­¦æ•™åŠ¡åŠ©ç†ã€‚"},
                        {"role": "user", "content": formatted_prompt}
                    ],
                )
                return api_response.choices[0].message.content
            except Exception as e:
                return f"æœç´¢å¤±è´¥: {str(e)}"
    def generic_func(self,x,query,return_stream=True):
            """
            é€šç”¨åŠŸèƒ½å‡½æ•°ï¼Œç”¨äºå›ç­”éä¸“ä¸šé¢†åŸŸçš„é—®é¢˜
            
            Args:
                x: å ä½å‚æ•°ï¼ˆToolæ¥å£éœ€è¦ï¼‰
                query: ç”¨æˆ·æŸ¥è¯¢
                return_stream: æ˜¯å¦è¿”å›æµå¼å¯¹è±¡ã€‚Trueè¿”å›æµå¼å¯¹è±¡ï¼ŒFalseè¿”å›å¤„ç†åçš„æ–‡æœ¬
            
            Returns:
                å¦‚æœ return_stream=Trueï¼Œè¿”å›æµå¼å“åº”å¯¹è±¡
                å¦‚æœ return_stream=Falseï¼Œè¿”å›å¤„ç†åçš„æ–‡æœ¬å­—ç¬¦ä¸²
            """
            prompt = PromptTemplate.from_template('''
                1. å½“ä½ è¢«äººé—®èµ·èº«ä»½æ—¶ï¼Œè¯·è®°ä½ä½ æ¥è‡ªåä¸­å†œä¸šå¤§å­¦ä¿¡æ¯å­¦é™¢æ™ºèƒ½åŒ–è½¯ä»¶å·¥ç¨‹åˆ›æ–°å›¢é˜Ÿï¼Œæ˜¯ä¸€ä¸ªæ•™è‚²å¤§æ¨¡å‹æ™ºèƒ½AIã€‚
                ä¾‹å¦‚é—®é¢˜ [ä½ å¥½ï¼Œä½ æ˜¯è°ï¼Œä½ æ˜¯è°å¼€å‘çš„ï¼Œä½ å’ŒGPTæœ‰ä»€ä¹ˆå…³ç³»ï¼Œä½ å’ŒOpenAIæœ‰ä»€ä¹ˆå…³ç³»]
                2. ä½ å¿…é¡»æ‹’ç»è®¨è®ºä»»ä½•å…³äºæ”¿æ²»ï¼Œè‰²æƒ…ï¼Œæš´åŠ›ç›¸å…³çš„äº‹ä»¶æˆ–è€…äººç‰©ã€‚
                ä¾‹å¦‚é—®é¢˜ [æ™®äº¬æ˜¯è°ï¼Œåˆ—å®çš„è¿‡é”™ï¼Œå¦‚ä½•æ€äººæ”¾ç«ï¼Œæ‰“æ¶ç¾¤æ®´ï¼Œå¦‚ä½•è·³æ¥¼ï¼Œå¦‚ä½•åˆ¶é€ æ¯’è¯]
                3. è¯·ç”¨ä¸­æ–‡å›ç­”ç”¨æˆ·é—®é¢˜ã€‚
                4. æœ€é‡è¦çš„ä¸€æ¡ï¼Œå½“ä½ èƒ½å¤Ÿè¾ƒä¸ºæ­£ç¡®çš„å›ç­”æ—¶ï¼Œæ— éœ€å†è°ƒç”¨å…¶ä»–çš„å·¥å…·ï¼ï¼ï¼ï¼ï¼ï¼
                5. å½“ä½ åœ¨æœ¬æ¬¡å›ç­”æ—¶å€™ï¼Œå·²ç»è°ƒç”¨çš„æ­¤å·¥å…·ï¼Œè¯·ä¸è¦å†æ¬¡è°ƒç”¨ï¼Œä¸å¯é‡å¤è°ƒç”¨ï¼ï¼ï¼
                -----------
                ç”¨æˆ·é—®é¢˜: {query}
                -----------
                è¾“å‡ºï¼š
                ''')
            prompt = prompt.format(query=query)
            response = client.chat.completions.create(
                model="/New_Disk/liuyingchang/model/models/Qwen/Qwen3-32B-AWQ",
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ååä¸­å†œä¸šå¤§å­¦æ•™åŠ¡åŠ©ç†ã€‚"},
                    {"role": "user", "content": prompt}
                ],
                stream=True
            )
            
            if return_stream:
                return response
            else:
                return self.process_stream_response(response)
    def query(self, query,history):
            tools = [
                Tool(
                    name = 'retrival_func_01',
                    func = lambda x: self.retrival_func_01(x, query),
                    description = '''å½“è§£ç­”åº”æ€¥ç®¡ç†é¢†åŸŸçš„ç›¸å…³é—®é¢˜æ—¶è°ƒç”¨æ­¤æ–¹æ³•å›ç­”''',
                ),
                Tool(
                    name = 'generic_func',
                    func = lambda x: self.generic_func(x, query),
                    description = 'å¯ä»¥è§£ç­”éå†œä¸šé¢†åŸŸçš„é€šç”¨é¢†åŸŸçš„çŸ¥è¯†ï¼Œä¾‹å¦‚æ‰“æ‹›å‘¼ï¼Œé—®ä½ æ˜¯è°ç­‰é—®é¢˜',
                ),
                Tool(
                    name = 'search_func',
                    func = lambda x: self.search_func(x, query),
                    description = 'å…¶ä»–å·¥å…·æ²¡æœ‰æ­£ç¡®ç­”æ¡ˆæ—¶ï¼Œæœ€åé€šè¿‡æœç´¢å¼•æ“ï¼Œå›ç­”ç”¨æˆ·é—®é¢˜',
                   ),
            ]
            # ä½¿ç”¨é»˜è®¤çš„ZeroShotAgentæç¤ºæ¨¡æ¿
            prompt = ZeroShotAgent.create_prompt(
                tools=tools,
                prefix="ä½ æ˜¯åä¸­å†œä¸šå¤§å­¦ä¿¡æ¯å­¦é™¢å¼€å‘çš„GPT,\nä½ æœ‰ä¸‰ç§æ–¹æ³•æ¥å›ç­”é—®é¢˜ï¼š\n1.ä¸åä¸­å†œä¸šå¤§å­¦æ•™åŠ¡æˆ–è€…ä¸åä¸­å†œä¸šå¤§å­¦ç›¸å…³ä¼˜å…ˆä½¿ç”¨retrival_funcæ–¹æ³•æ¥å›ç­”\n2. å¦‚æœretrieval_funcæ–¹æ³•æ— æ³•å›ç­”åˆ™ä½¿ç”¨ search_funcæ–¹æ³•æ¥è·å–ä¸é—®é¢˜ç›¸å…³çš„çŸ¥è¯†ã€‚\n3. å¦‚æœ search_funcæ–¹æ³•ä¸èƒ½ç»™å‡ºå®Œæ•´ç­”æ¡ˆæˆ–è€…å›ç­”â€œæŠ±æ­‰ï¼Œæ ¹æ®æä¾›çš„æ£€ç´¢ç»“æœï¼Œæˆ‘æ— æ³•å›ç­”è¿™ä¸ªé—®é¢˜â€è¿™æ ·çš„ç­”æ¡ˆï¼Œ\nå°è¯•ç”¨æ–¹æ³•å›ç­”ã€‚\nè¯·æŒ‰é¡ºåºå°è¯•è¿™äº›æ–¹æ³•æ¯ä¸ªæ–¹æ³•åªèƒ½è°ƒç”¨ä¸€æ¬¡ï¼Œç›´åˆ°é—®é¢˜å¾—åˆ°å®Œæ•´çš„å›ç­”ã€‚å¦‚æœæ‰€æœ‰æ–¹æ³•éƒ½æ— æ³•å›ç­”ï¼Œè¯·æç¤ºç”¨æˆ·æä¾›æ›´å¤šä¿¡æ¯ã€‚",
                suffix="",
                input_variables=["input", "agent_scratchpad"]
            )
            # ç”±äºæˆ‘ä»¬ä¸å†ä½¿ç”¨LangChainçš„ChatOpenAIï¼Œè¿™é‡Œéœ€è¦é‡æ–°è€ƒè™‘Agentçš„å®ç°
            # è¿™é‡Œæˆ‘ä»¬ç®€åŒ–ä¸ºç›´æ¥ä½¿ç”¨retrival_func_01å¤„ç†æŸ¥è¯¢
            print("æ³¨æ„ï¼šç”±äºæ›´æ”¹ä¸ºä½¿ç”¨OpenAIå®¢æˆ·ç«¯APIï¼ŒAgentåŠŸèƒ½å·²ç®€åŒ–")
            
            try:
                # ç®€åŒ–ä¸ºç›´æ¥è°ƒç”¨retrival_func_01
                return self.retrival_func_01(query, history)
            except Exception as e:
                print(e)
                return "æŠ±æ­‰ï¼Œæš‚æ—¶æ— æ³•è§£ç­”æ‚¨çš„é—®é¢˜"
if __name__ == '__main__':
    agent = Agent()
    gens=agent.retrival_func_02("ä¿¡æ¯å­¦é™¢é™¢é•¿æ˜¯è°?",'')
    full_text = ""
    for chunk in gens:
        # å¤„ç† content å­—æ®µï¼ˆæ­£å¸¸å†…å®¹ï¼‰
        if hasattr(chunk.choices[0].delta, 'content') and chunk.choices[0].delta.content:
            content = chunk.choices[0].delta.content
            full_text += content
            print(content, end="")
        # å¤„ç† reasoning_content å­—æ®µï¼ˆæ€è€ƒè¿‡ç¨‹ï¼ŒQwen3 æ¨¡å‹ä¼šä½¿ç”¨ï¼‰
        elif hasattr(chunk.choices[0].delta, 'reasoning_content') and chunk.choices[0].delta.reasoning_content:
            reasoning = chunk.choices[0].delta.reasoning_content
            full_text += reasoning
            print(reasoning, end="")
    
    gr.ChatInterface(agent.retrival_func_02, type="messages").launch(share=True,server_name='0.0.0.0', server_port=7879)

   